{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gentle introduction to OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optical character recognition hoặc OCR là một trong những nhiệm vụ thị giác máy tính được giải quyết sớm nhất, vì ở một số khía cạnh, nó không yêu cầu học sâu. Do đó, đã có những cách triển khai OCR khác nhau ngay cả trước khi bùng nổ học sâu vào năm 2012 và một số thậm chí có từ năm 1914."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều này khiến nhiều người nghĩ rằng thử thách OCR đã được “giải quyết”, nó không còn là thử thách nữa. Một số khác tin rằng OCR không yêu cầu học sâu, hay nói cách khác, sử dụng học sâu cho OCR là một việc làm quá mức cần thiết.\n",
    "\n",
    "Bất kỳ ai thực hành thị giác máy tính, hoặc học máy nói chung, đều biết rằng không có thứ gì được coi là một nhiệm vụ đã được giải quyết và trường hợp này cũng không khác. Ngược lại, OCR chỉ mang lại kết quả rất tốt trong các trường hợp sử dụng rất cụ thể, nhưng nói chung, nó vẫn được coi là thách thức.\n",
    "\n",
    "Ngoài ra, đúng là có những giải pháp tốt cho một số tác vụ OCR không yêu cầu học sâu. Tuy nhiên, để thực sự tiến tới những giải pháp tốt hơn, tổng thể hơn, việc học sâu sẽ là điều bắt buộc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you’ll find here\n",
    "Trong bài đăng này, tôi sẽ khám phá một số **chiến lược**, **phương pháp** và **logic** được sử dụng để giải quyết các nhiệm vụ OCR khác nhau và sẽ chia sẻ một số cách tiếp cận hữu ích. Trong phần cuối cùng, chúng ta sẽ giải quyết một vấn đề trong **thế giới thực** với mã. Đây không nên được coi là một đánh giá toàn diện vì chiều sâu, lịch sử và bề rộng của các phương pháp tiếp cận là quá rộng đối với loại bài đăng trên blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of OCR\n",
    "Như đã gợi ý trước đây, có nhiều hơn một ý nghĩa cho OCR. Theo nghĩa chung nhất của nó, nó đề cập đến việc trích xuất văn bản từ mọi hình ảnh có thể có, có thể là một trang in tiêu chuẩn từ một cuốn sách hoặc một hình ảnh ngẫu nhiên có hình vẽ bậy trong đó (“trong tự nhiên”). Ở giữa, bạn có thể tìm thấy nhiều tác vụ khác, chẳng hạn như đọc biển số xe, hình ảnh xác thực không có rô-bốt, biển báo đường phố, v.v.\n",
    "Mặc dù mỗi lựa chọn này đều có những khó khăn riêng, nhưng rõ ràng nhiệm vụ “trong tự nhiên” là khó nhất.\n",
    "\n",
    "![](images/1.png)\n",
    "*Left: Printed text. Right: text in the wild*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ các ví dụ này, chúng ta có thể rút ra một số thuộc tính của các nhiệm vụ OCR:\n",
    "- **Mật độ văn bản (Text density)**: trên một trang in / viết, văn bản dày đặc. Tuy nhiên, với hình ảnh một con phố chỉ có một biển báo, dòng chữ rất thưa thớt.\n",
    "- **Cấu trúc của văn bản (Structure of text)**: văn bản trên một trang được cấu trúc, hầu hết theo các hàng nghiêm ngặt, trong khi văn bản trong tự nhiên có thể được rải khắp mọi nơi, theo các cách xoay vòng khác nhau.\n",
    "- **Phông chữ (Fonts)**: phông chữ in dễ dàng hơn, vì chúng có cấu trúc hơn so với các ký tự viết tay ngoằn ngoèo.\n",
    "- **Loại ký tự (Character type)**: văn bản có thể có ngôn ngữ khác nhau, có thể rất khác nhau. Ngoài ra, cấu trúc của văn bản có thể khác với số, chẳng hạn như số nhà, v.v.\n",
    "- **Vị trí (Location)**: một số tác vụ bao gồm văn bản được cắt xén / căn giữa, trong khi ở những tác vụ khác, văn bản có thể nằm ở các vị trí ngẫu nhiên trong hình ảnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data sets/Tasks\n",
    "### 2.1 SVHN\n",
    "Một nơi tốt để bắt đầu là [SVHN](http://ufldl.stanford.edu/housenumbers/), tập dữ liệu Số nhà ở Chế độ xem phố. Như tên gọi của nó, đây là tập dữ liệu về số nhà được trích xuất từ chế độ xem phố của google. Khó khăn của nhiệm vụ là trung gian. Các chữ số có nhiều hình dạng và kiểu viết khác nhau, tuy nhiên, mỗi số nhà nằm ở giữa hình ảnh, do đó không cần phát hiện. Hình ảnh không có độ phân giải cao và cách sắp xếp của chúng có thể hơi kỳ lạ\n",
    "\n",
    "![](images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 License plates\n",
    "Một thách thức phổ biến khác, không khó và hữu ích trong thực tế, là nhận dạng biển số xe. Tác vụ này, giống như hầu hết các tác vụ OCR, yêu cầu phát hiện biển số và sau đó nhận dạng các ký tự của biển số đó. Vì hình dạng của tấm tương đối không đổi, một số phương pháp sử dụng phương pháp định hình lại đơn giản trước khi thực sự nhận ra các chữ số. Dưới đây là một số ví dụ từ web:\n",
    "\n",
    "![](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [OpenALPR](https://github.com/openalpr/openalpr) là một công cụ rất mạnh mẽ, không liên quan đến học sâu, để nhận dạng biển số xe từ các quốc gia khác nhau\n",
    "2. [Repo](https://github.com/qjadud1994/CRNN-Keras) này cung cấp việc triển khai mô hình CRNN (sẽ được thảo luận thêm) để nhận dạng biển số xe Hàn Quốc.\n",
    "3. [Supervise.ly](https://towardsdatascience.com/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c), một công ty tiện ích dữ liệu, đã viết về việc đào tạo một người nhận dạng biển số xe bằng cách sử dụng dữ liệu nhân tạo do công cụ của họ tạo ra (dữ liệu nhân tạo cũng sẽ được thảo luận thêm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CAPTCHA\n",
    "Vì Internet có đầy rẫy rô-bốt, một thực tế phổ biến để phân biệt chúng với con người thực, là các nhiệm vụ thị giác, cụ thể là đọc văn bản, hay còn gọi là CAPTCHA. Nhiều văn bản trong số này là ngẫu nhiên và bị bóp méo, khiến máy tính khó đọc hơn. Tôi không chắc ai đã phát triển CAPTCHA đã dự đoán những tiến bộ trong thị giác máy tính, tuy nhiên hầu hết ngày nay, CAPTCHA dạng văn bản không quá khó để giải quyết, đặc biệt nếu chúng tôi không cố gắng giải tất cả chúng cùng một lúc. \n",
    "\n",
    "![](images/4.png)\n",
    "*Facebook knows how to make challenging CAPTCHAs*\n",
    "\n",
    "Adam Geitgey cung cấp một [hướng dẫn hay](https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710) để giải một số CAPTCHA bằng học sâu, bao gồm tổng hợp dữ liệu nhân tạo một lần nữa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PDF OCR\n",
    "Kịch bản phổ biến nhất cho OCR là OCR in / pdf. Bản chất có cấu trúc của tài liệu in giúp việc phân tích chúng dễ dàng hơn nhiều. Hầu hết các công cụ OCR (ví dụ: [Tesseract](https://github.com/tesseract-ocr/)) chủ yếu nhằm giải quyết nhiệm vụ này và đạt được kết quả tốt. Vì vậy, tôi sẽ không nói quá nhiều về nhiệm vụ này trong bài đăng này"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 OCR in the wild\n",
    "Đây là nhiệm vụ OCR thách thức nhất, vì nó đưa tất cả các thách thức về thị giác máy tính nói chung như tiếng ồn, ánh sáng và hiện vật vào OCR. Một số tập dữ liệu có liên quan cho tác vụ này là [coco-text](https://vision.cornell.edu/se3/coco-text-2/) và tập dữ liệu [SVT](http://tc11.cvc.uab.es/datasets/SVT_1) một lần nữa sử dụng hình ảnh chế độ xem phố để trích xuất văn bản.\n",
    "\n",
    "![](images/5.png)\n",
    "\n",
    "*COCO text example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Synth text\n",
    "SynthText không phải là một tập dữ liệu và có lẽ thậm chí không phải là một nhiệm vụ, nhưng một ý tưởng hay để cải thiện hiệu quả đào tạo là tạo dữ liệu nhân tạo. Việc ném các ký tự hoặc từ ngẫu nhiên lên một hình ảnh sẽ có vẻ tự nhiên hơn nhiều so với bất kỳ đối tượng nào khác, vì bản chất phẳng của văn bản.\n",
    "\n",
    "Chúng tôi đã thấy trước đó một số tạo dữ liệu cho các tác vụ dễ dàng hơn như CAPTCHA và biển số xe. Tạo văn bản tự nhiên phức tạp hơn một chút. Nhiệm vụ bao gồm việc xem xét thông tin chiều sâu của một hình ảnh. May mắn thay, SynthText là một tác phẩm tuyệt vời có hình ảnh với các chú thích nói trên và rắc các từ một cách thông minh (từ tập dữ liệu nhóm tin).\n",
    "\n",
    "![](images/6.png)\n",
    "\n",
    "*Hình minh họa quy trình SynthText: phía trên bên phải là phân đoạn của hình ảnh, phía dưới bên phải là dữ liệu độ sâu. Dưới cùng bên trái là phần phân tích bề mặt của hình ảnh, theo văn bản được rải trên hình ảnh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để làm cho văn bản “sprinkled” trông thực tế và hữu ích, thư viện SynthText sử dụng mỗi hình ảnh hai mặt nạ, một mặt nạ chiều sâu và một mặt nạ phân đoạn khác. Nếu bạn muốn sử dụng hình ảnh của riêng mình, bạn cũng nên thêm dữ liệu này.\n",
    "\n",
    "> Bạn nên kiểm tra [repo](https://github.com/ankush-me/SynthText) và tự tạo một số hình ảnh. Bạn nên chú ý rằng repo sử dụng một số phiên bản opencv và maptlotlib đã lỗi thời, vì vậy một số sửa đổi có thể là cần thiết.\n",
    "\n",
    "![](images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Mnist\n",
    "Mặc dù không thực sự là một nhiệm vụ OCR, nhưng không thể viết về OCR và không bao gồm ví dụ Mnist. Thử thách thị giác máy tính được biết đến nhiều nhất không thực sự là một nhiệm vụ OCR và được cân nhắc, vì nó chỉ chứa một ký tự (chữ số) tại một thời điểm và chỉ có 10 chữ số. Tuy nhiên, nó có thể gợi ý tại sao OCR được coi là dễ dàng. Ngoài ra, trong một số phương pháp tiếp cận, mỗi chữ cái sẽ được phát hiện riêng biệt, và sau đó các mô hình giống như (phân loại) của Mnist trở nên có liên quan.\n",
    "\n",
    "![](images/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strategies\n",
    "Như chúng ta đã thấy và ngụ ý, việc nhận dạng văn bản chủ yếu là một nhiệm vụ gồm hai bước. Trước tiên, bạn muốn phát hiện (các) văn bản xuất hiện trong hình ảnh, nó có thể dày đặc (như trong tài liệu in) hay thưa thớt (Như văn bản trong tự nhiên).\n",
    "\n",
    "Sau khi phát hiện mức độ dòng / từ, chúng ta có thể chọn lại một lần nữa từ một tập hợp lớn các giải pháp, thường xuất phát từ ba cách tiếp cận chính:\n",
    "1. Kỹ thuật thị giác máy tính cổ điển.\n",
    "2. Học sâu chuyên biệt.\n",
    "3. Phương pháp tiếp cận học sâu tiêu chuẩn (Phát hiện).\n",
    "\n",
    "Chúng ta hãy xem xét từng người trong số họ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 1. Classic computer vision techniques\n",
    "Như đã nói trước đó, thị giác máy tính đã giải quyết các vấn đề nhận dạng văn bản khác nhau trong một thời gian dài. Bạn có thể tìm thấy nhiều ví dụ trực tuyến:\n",
    "\n",
    "- **Adrian Rosebrook** có rất nhiều hướng dẫn trên trang web của mình, như [bài này](https://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/), [bài này](https://www.pyimagesearch.com/2017/07/24/bank-check-ocr-with-opencv-and-python-part-i/) và [nhiều bài khác](https://www.pyimagesearch.com/category/optical-character-recognition-ocr/).\n",
    "- **Stack overflow** cũng có một số [bài viết](https://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương pháp CV cổ điển thường tiếp cận theo cách:\n",
    "1. Áp dụng các **bộ lọc** để làm cho các ký tự nổi bật so với nền.\n",
    "2. Áp dụng **contour detection** để nhận dạng từng ký tự một.\n",
    "3. Áp dụng **phân loại hình ảnh** để xác định các ký tự\n",
    "\n",
    "Rõ ràng, nếu phần hai được thực hiện tốt, phần ba sẽ dễ dàng với khớp mẫu hoặc học máy (ví dụ: Mnist).\n",
    "\n",
    "Tuy nhiên, việc phát hiện đường viền là khá khó khăn cho việc tổng quát hóa. nó đòi hỏi rất nhiều tinh chỉnh thủ công, do đó trở nên không khả thi trong hầu hết các vấn đề. Ví dụ: hãy áp dụng một tập lệnh thị giác máy tính đơn giản từ đây trên một số hình ảnh từ tập dữ liệu SVHN. Ở lần thử đầu tiên, chúng tôi có thể đạt được kết quả rất tốt:\n",
    "\n",
    "![](images/9.png) ![](images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhưng khi các nhân vật ở gần nhau hơn, mọi thứ bắt đầu vỡ lẽ:\n",
    "![](images/11.png) ![](images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tôi đã tìm ra một cách khó, đó là khi bạn bắt đầu lộn xộn với các thông số, bạn có thể giảm những lỗi như vậy, nhưng không may lại gây ra cho những người khác. Nói cách khác, nếu nhiệm vụ của bạn không đơn giản, các phương pháp này không phải là cách để thực hiện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Specialized deep learning approaches\n",
    "Hầu hết các phương pháp học sâu thành công đều vượt trội về tính tổng quát của chúng. Tuy nhiên, xem xét các thuộc tính được mô tả ở trên, Mạng chuyên biệt có thể rất hữu ích.\n",
    "\n",
    "Tôi sẽ xem xét ở đây một mẫu không đầy đủ về một số cách tiếp cận nổi bật và sẽ thực hiện một bản tóm tắt rất nhanh về các bài báo trình bày chúng. Như mọi khi, mọi bài báo đều được mở đầu bằng từ “nhiệm vụ X (nhận dạng văn bản) gần đây thu hút được sự chú ý” và tiếp tục mô tả chi tiết phương pháp của họ. Đọc kỹ các bài báo sẽ thấy các phương pháp này được ghép từ các phần của các công trình học sâu / nhận dạng văn bản trước đó.\n",
    "\n",
    "Kết quả cũng được mô tả kỹ lưỡng, tuy nhiên do có nhiều khác biệt trong thiết kế (bao gồm cả sự khác biệt nhỏ trong bộ dữ liệu) so sánh thực tế là hoàn toàn không thể. Cách duy nhất để thực sự biết hiệu suất của các phương pháp này trong nhiệm vụ của bạn là lấy mã của chúng (tốt nhất là tệ hơn: tìm repo chính thức, tìm repo không chính thức nhưng được đánh giá cao, tự thực hiện) và thử nó trên dữ liệu của bạn.\n",
    "\n",
    "Vì vậy, chúng tôi sẽ luôn ưu tiên các bài viết có repos tốt đi kèm, và thậm chí cả demo nếu có thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAST\n",
    "**[EAST](https://arxiv.org/pdf/1704.03155.pdf) ( Efficient accurate scene text detector)** là một cách tiếp cận đơn giản nhưng mạnh mẽ để phát hiện văn bản. Sử dụng mạng chuyên biệt.\n",
    "\n",
    "Không giống như các phương pháp khác mà chúng ta sẽ thảo luận, chỉ giới hạn trong việc phát hiện văn bản (không phải nhận dạng thực tế), tuy nhiên, tính mạnh mẽ của nó khiến nó đáng được đề cập."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một ưu điểm nữa là nó cũng đã được thêm vào thư viện open-CV (từ phiên bản 4) để bạn có thể dễ dàng sử dụng (xem hướng dẫn [tại đây](https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/)).\n",
    "Mạng thực sự là một phiên bản của **U-Net** nổi tiếng, tốt cho việc phát hiện các tính năng có thể khác nhau về kích thước. Nguồn cấp dữ liệu cơ bản “gốc” chuyển tiếp (như được đặt ra trong bài viết, xem hình bên dưới) của mạng này rất có thể - **PVANet** được sử dụng trong bài báo, tuy nhiên việc triển khai opencv lại sử dụng **Resnet**. Rõ ràng, nó cũng có thể được đào tạo trước (với imagenet, ví dụ:). Như trong U-Net, các tính năng được trích xuất từ các cấp khác nhau trong mạng.\n",
    "\n",
    "![](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, mạng cho phép hai loại hộp giới hạn xoay đầu ra: hộp giới hạn tiêu chuẩn với góc quay (tham số 2X2 + 1) hoặc \"tứ giác\" chỉ là một hộp giới hạn xoay với tọa độ của tất cả các đỉnh.\n",
    "\n",
    "![](images/14.png)\n",
    "\n",
    "Nếu kết quả cuộc sống thực sẽ giống như trong những hình ảnh trên, việc nhận dạng các văn bản sẽ không tốn nhiều công sức. Tuy nhiên, kết quả ảnh cuộc sống thực không hoàn hảo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRNN\n",
    "**Convolutional-recurrent neural network**, là một bài báo từ năm 2015, đề xuất kiến trúc kết hợp (hoặc tribrid?) End to end, nhằm ghi lại các từ, theo cách tiếp cận ba bước.\n",
    "\n",
    "Ý tưởng diễn ra như sau: mức đầu tiên là một mạng tích tụ đầy đủ tiêu chuẩn. Lớp cuối cùng của mạng được định nghĩa là lớp đặc trưng và được chia thành \"cột tính năng\". Hãy xem trong hình ảnh bên dưới cách mọi cột tính năng như vậy nhằm thể hiện một phần nhất định trong văn bản.\n",
    "\n",
    "![](images/15.png)\n",
    "\n",
    "Sau đó, các cột tính năng được đưa vào một **deep-bidirectional LSTM** để xuất ra một chuỗi và nhằm mục đích tìm kiếm mối quan hệ giữa các ký tự.\n",
    "\n",
    "![](images/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, phần thứ ba là một  transcription layer. Mục tiêu của nó là lấy chuỗi ký tự lộn xộn, trong đó một số ký tự bị thừa và những ký tự khác trống, và sử dụng phương pháp xác suất để thống nhất (probabilistic) để tạo từ có ý nghĩa.\n",
    "\n",
    "Phương pháp này được gọi là **CTC loss**, và có thể đọc ở [đây](https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fgab41.lab41.org%2Fspeech-recognition-you-down-with-ctc-8d3b558943f0). Lớp này có thể được sử dụng với / không có từ vựng được xác định trước, điều này có thể tạo điều kiện cho các dự đoán từ.\n",
    "\n",
    "Bài báo này đạt tỷ lệ chính xác cao (> 95%) với từ vựng văn bản cố định và tỷ lệ thành công khác nhau nếu không có nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STN-net/SEE\n",
    "**[SEE](https://arxiv.org/pdf/1712.05404.pdf) — Semi-Supervised End-to-End Scene Text Recognition**, là một tác phẩm của Christian Bartzi. Ông và các đồng nghiệp của mình áp dụng chiến lược thực sự kết thúc để phát hiện và nhận dạng văn bản. Họ sử dụng giám sát rất yếu (mà họ gọi là giám sát bán phần, theo một nghĩa khác với thông thường). khi họ huấn luyện mạng chỉ với **text annotation** (không có hộp giới hạn). Điều này cho phép họ sử dụng nhiều dữ liệu hơn, nhưng làm cho quy trình đào tạo của họ khá khó khăn và họ thảo luận về các thủ thuật khác nhau để làm cho nó hoạt động, ví dụ: không đào tạo trên hình ảnh có nhiều hơn hai dòng văn bản (ít nhất là ở giai đoạn đào tạo đầu tiên)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài báo có một phiên bản cũ hơn được gọi là [STN OCR](https://arxiv.org/abs/1707.08831). Trong bài báo cuối cùng, các nhà nghiên cứu đã tinh chỉnh các phương pháp và cách trình bày của họ, đồng thời họ cũng nhấn mạnh hơn vào tính tổng quát của phương pháp tiếp cận dựa trên chất lượng cao của kết quả.\n",
    "\n",
    "![](images/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tên **STN-OCR** gợi ý về chiến lược sử dụng [spatial transformer](https://arxiv.org/pdf/1506.02025.pdf) (= STN, không liên quan đến máy biến áp google transformer).\n",
    "\n",
    "Họ đào tạo **hai mạng ghép nối**, trong đó mạng đầu tiên, transformer, học một phép biến đổi trên hình ảnh để tạo ra một hình ảnh con dễ hiểu hơn.\n",
    "\n",
    "Sau đó, một mạng chuyển tiếp nguồn cấp dữ liệu khác với LSTM ở trên cùng (hmm… có vẻ như chúng ta đã thấy nó trước đây) để nhận dạng văn bản.\n",
    "\n",
    "Các nghiên cứu nhấn mạnh ở đây tầm quan trọng của việc sử dụng resnet (họ sử dụng nó hai lần) vì nó cung cấp sự lan truyền “mạnh mẽ” cho các lớp ban đầu. tuy nhiên thực tế này khá được chấp nhận ngày nay.\n",
    "\n",
    "Dù bằng cách nào, đây cũng là một cách tiếp cận thú vị để thử."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Standard deep learning approach\n",
    "Như tiêu đề ngụ ý, sau khi phát hiện các “từ”, chúng tôi có thể áp dụng các phương pháp phát hiện học sâu tiêu chuẩn, chẳng hạn như SSD, YOLO và Mask RCNN. Tôi sẽ không trình bày quá nhiều về các phương pháp tiếp cận luận văn vì có rất nhiều thông tin trực tuyến.\n",
    "\n",
    "Tôi phải nói rằng đây hiện là cách tiếp cận yêu thích của tôi, vì những gì tôi thích trong học sâu là triết lý \"kết thúc để kết thúc\", nơi bạn áp dụng một mô hình mạnh mẽ với một số điều chỉnh sẽ giải quyết hầu hết mọi vấn đề. Trong phần tiếp theo của bài đăng này, chúng ta sẽ xem nó thực sự hoạt động như thế nào.\n",
    "\n",
    "Tuy nhiên, SSD và các mô hình phát hiện khác bị thách thức khi nói đến các lớp tương tự, dày đặc, như được xem xét ở [đây](https://arxiv.org/pdf/1611.10012.pdf). Tôi thấy hơi mỉa mai vì trên thực tế, các mô hình học sâu khó nhận ra các chữ số và chữ cái hơn nhiều so với việc nhận ra các đối tượng phức tạp và khó khăn hơn nhiều như chó, mèo hoặc người. Chúng có xu hướng không đạt được độ chính xác mong muốn, và do đó, các phương pháp tiếp cận chuyên biệt phát triển mạnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A gentle introduction to OCR - Gidi Shperber](https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
