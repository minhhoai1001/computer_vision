{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Tracking with mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The ability to perceive the shape and motion of hands can be a vital component in improving the user experience across a variety of technological domains and platforms. For example, it can form the basis for sign language understanding and hand gesture control, and can also enable the overlay of digital content and information on top of the physical world in augmented reality. While coming naturally to people, robust real-time hand perception is a decidedly challenging computer vision task, as hands often occlude themselves or each other (e.g. finger/palm occlusions and hand shakes) and lack high contrast patterns.\n",
    "\n",
    "MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from just a single frame. Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference, our method achieves real-time performance on a mobile phone, and even scales to multiple hands. We hope that providing this hand perception functionality to the wider research and development community will result in an emergence of creative use cases, stimulating new applications and new research avenues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "MediaPipe Hands utilizes an ML pipeline consisting of multiple models working together: A palm detection model that operates on the full image and returns an oriented hand bounding box. A hand landmark model that operates on the cropped image region defined by the palm detector and returns high-fidelity 3D hand keypoints. This strategy is similar to that employed in our [MediaPipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html) solution, which uses a face detector together with a face landmark model.\n",
    "\n",
    "Providing the accurately cropped hand image to the hand landmark model drastically reduces the need for data augmentation (e.g. rotations, translation and scale) and instead allows the network to dedicate most of its capacity towards coordinate prediction accuracy. In addition, in our pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand.\n",
    "\n",
    "The pipeline is implemented as a MediaPipe [graph](https://github.com/google/mediapipe/blob/master/mediapipe/graphs/hand_tracking/hand_tracking_mobile.pbtxt) that uses a [hand landmark tracking](https://github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark_tracking_gpu.pbtxt) subgraph from the [hand landmark module](https://github.com/google/mediapipe/tree/master/mediapipe/modules/hand_landmark), and renders using a dedicated [hand renderer subgraph](https://github.com/google/mediapipe/blob/master/mediapipe/graphs/hand_tracking/subgraphs/hand_renderer_gpu.pbtxt). The [hand landmark tracking subgraph](https://github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark_tracking_gpu.pbtxt) internally uses a [hand landmark subgraph](https://github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark_gpu.pbtxt) from the same module and a [palm detection subgraph](https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection_gpu.pbtxt) from the [palm detection module](https://github.com/google/mediapipe/tree/master/mediapipe/modules/palm_detection).\n",
    "\n",
    "Note: To visualize a graph, copy the graph and paste it into [MediaPipe Visualizer](https://viz.mediapipe.dev/). For more information on how to visualize its associated subgraphs, please see [visualizer documentation](https://google.github.io/mediapipe/tools/visualizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Palm Detection Model\n",
    "To detect initial hand locations, we designed a [single-shot detector](https://arxiv.org/abs/1512.02325) model optimized for mobile real-time uses in a manner similar to the face detection model in [MediaPipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html). Detecting hands is a decidedly complex task: our [model](https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection.tflite) has to work across a variety of hand sizes with a large scale span (~20x) relative to the image frame and be able to detect occluded and self-occluded hands. Whereas faces have high contrast patterns, e.g., in the eye and mouth region, the lack of such features in hands makes it comparatively difficult to detect them reliably from their visual features alone. Instead, providing additional context, like arm, body, or person features, aids accurate hand localization.\n",
    "\n",
    "Our method addresses the above challenges using different strategies. First, we train a *palm detector* instead of a hand detector, since estimating bounding boxes of rigid objects like palms and fists is significantly simpler than detecting hands with articulated fingers. In addition, as palms are *smaller objects*, the non-maximum suppression algorithm works well even for two-hand self-occlusion cases, like handshakes. Moreover, palms can be modelled using *square bounding boxes* (anchors in ML terminology) ignoring other aspect ratios, and therefore reducing the number of anchors by a factor of 3-5. Second, an *encoder-decoder feature extractor* is used for bigger scene context awareness even for small objects (similar to the RetinaNet approach). Lastly, we *minimize the focal loss* during training to support a large amount of anchors resulting from the high scale variance.\n",
    "\n",
    "With the above techniques, we achieve an average precision of **95.7%** in palm detection. Using a regular cross entropy loss and no decoder gives a baseline of just **86.22%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Landmark Model\n",
    "After the palm detection over the whole image our subsequent hand landmark model performs precise keypoint localization of 21 3D hand-knuckle coordinates inside the detected hand regions via regression, that is direct coordinate prediction. The model learns a consistent internal hand pose representation and is robust even to partially visible hands and self-occlusions.\n",
    "\n",
    "To obtain ground truth data, we have manually annotated ~30K real-world images with 21 3D coordinates, as shown below (we take Z-value from image depth map, if it exists per corresponding coordinate). To better cover the possible hand poses and provide additional supervision on the nature of hand geometry, we also render a high-quality synthetic hand model over various backgrounds and map it to the corresponding 3D coordinates.\n",
    "\n",
    "![](https://google.github.io/mediapipe/images/mobile/hand_landmarks.png)\n",
    "\n",
    "<center>Fig 2. 21 hand landmarks.</center>\n",
    "\n",
    "![](https://google.github.io/mediapipe/images/mobile/hand_crops.png)\n",
    "\n",
    "<center>Fig 3. Top: Aligned hand crops passed to the tracking network with ground truth annotation. Bottom: Rendered synthetic hand images with ground truth annotation.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
